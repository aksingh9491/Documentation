### RAID Levels:
####RAID0:
RAID 0 is called disk striping. All the data is spread out in chunks across all the disks in the RAID set. RAID 0 has great performance because you spread the load of storing data onto more physical drives. There is no parity generated for RAID 0. Therefore there is no overhead to write data to RAID 0 disks. RAID 0 is only good for better performance, and not for high availability, since parity is not generated for RAID 0 disks. RAID 0 requires at least two physical disks.

Minimum 2 disks.
Excellent performance ( as blocks are striped ).
No redundancy ( no mirror, no parity ).
Don’t use this for any critical system.

Advantages:

RAID 0 offers great performance, both in read and write operations. There is no overhead caused by parity controls.
All storage capacity is used, there is no overhead.
The technology is easy to implement.

Disadvantages:

RAID 0 is not fault-tolerant. If one drive fails, all data in the RAID 0 array are lost. It should not be used for mission-critical systems.

####RAID 1:
RAID 1 is called disk mirroring. All the data is written to at least two separate physical disks. The disks are essentially mirror images of each other. If one of the disks fails, the other can be used to retrieve data. Disk mirroring is good for very fast read operations. It's slower when writing to the disks, since the data needs to be written twice. RAID 1 requires at least two physical disks.
Minimum 2 disks.
Good performance ( no striping. no parity ).
Excellent redundancy ( as blocks are mirrored ).

Advantages:

RAID 1 offers excellent read speed and a write-speed that is comparable to that of a single drive.
In case a drive fails, data do not have to be rebuild, they just have to be copied to the replacement drive.
RAID 1 is a very simple technology.

Disadvantages:

The main disadvantage is that the effective storage capacity is only half of the total drive capacity because all data get written twice.
Software RAID 1 solutions do not always allow a hot swap of a failed drive. That means the failed drive can only be replaced after powering down the computer it is attached to. For servers that are used simultaneously by many people, this may not be acceptable. Such systems typically use hardware controllers that do support hot swapping.

#### RAID 2: No Longer USed

#### RAID3:
RAID 3 uses something called a parity disk to store the parity information generated by the RAID controller on a separate disk from the actual data disks, instead of striping it with the data as in RAID 5. This RAID type is not currently used very often because it performs poorly when there are a lot of little requests for data, as in a database. This type performs well under applications that just want one long sequential data transfer. Applications like video servers work well with this RAID type. RAID 3 requires a minimum of three physical disks.

#### RAID 4: No Longer Used:
###RAID 5:
RAID 5 uses disk striping with parity. The data is striped across all the disks in the RAID set, along with the parity information needed to reconstruct the data in case of disk failure. RAID 5 is the most common method used, since it achieves a good balance between performance and availability. RAID 5 requires at least three physical disks.
Minimum 3 disks.
Good performance ( as blocks are striped ).
Good redundancy ( distributed parity ).
Best cost effective option providing both performance and redundancy. Use this for DB that is heavily read oriented. Write operations will be slow.

Advantages:

Read data transactions are very fast while write data transactions are somewhat slower (due to the parity that has to be calculated).
If a drive fails, you still have access to all data, even while the failed drive is being replaced and the storage controller rebuilds the data on the new drive.

Disadvantages:

Drive failures have an effect on throughput, although this is still acceptable.
This is complex technology. If one of the disks in an array using 4TB disks fails and is replaced, restoring the data (the rebuild time) may take a day or longer, depending on the load on the array and the speed of the controller. If another disk goes bad during that time, data are lost forever.

####RAID 6 :
 RAID 6 increases reliability by utilizing two parity stripes, which allows for two disk failures within the RAID set before data is lost. RAID 6 is seen in SATA environments, and solutions that require long data retention periods, such as data archiving or disk-based backup.

Advantages:

Like with RAID 5, read data transactions are very fast.
If two drives fail, you still have access to all data, even while the failed drives are being replaced. So RAID 6 is more secure than RAID 5.

Disadvantages:

Write data transactions are slowed down due to the parity that has to be calculated.
Drive failures have an effect on throughput, although this is still acceptable.
This is complex technology. Rebuilding an array in which one drive failed can take a long time.
####RAID 1 + 0:
RAID 1+0, which is also called RAID 10, uses a combination of disk mirroring and disk striping. The data is normally mirrored first and then striped. Mirroring striped sets accomplishes the same task, but is less fault tolerant than striping mirror sets. If you lose a drive in a stripe set, all access to data must be from the other stripe set, because stripe sets have no parity. RAID 1+0 requires a minimum of four physical disks.

Minimum 4 disks.
This is also called as “stripe of mirrors”
Excellent redundancy ( as blocks are mirrored )
Excellent performance ( as blocks are striped )
If you can afford the dollar, this is the BEST option for any mission critical applications (especially databases).

Advantages:

If something goes wrong with one of the disks in a RAID 10 configuration, the rebuild time is very fast since all that is needed is copying all the data from the surviving mirror to a new drive. This can take as little as 30 minutes for drives of  1 TB.

Disadvantages:

Half of the storage capacity goes to mirroring, so compared to large RAID 5  or RAID 6 arrays, this is an expensive way to have redundancy.

NOTE: All RAID levels except RAID 0 offer protection from a single drive failure. A RAID 6 system even survives 2 disks dying simultaneously. For complete security you do still need to back-up the data from a RAID system.

That back-up will come in handy if all drives fail simultaneously because of a power spike.
It is a safeguard when the storage system gets stolen.
Back-ups can be kept off-site at a different location. This can come in handy if a natural disaster or fire destroys your workplace.
The most important reason to back-up multiple generations of data is user error. If someone accidentally deletes some important data and this goes unnoticed for several hours, days or weeks, a good set of back-ups ensure you can still retrieve those files.

### Software RAID:

You can configure RAID at the time of OS installation or you can use hardware raid. There are few commands if we are going to use software raid:

Considering number of dedicated server rentals that just gives JBOD, setting up your own software raid is quite handy. This tutorial goes over the very basic of how it’s done.

All of this should be done under root.
Let’s say you have 3 disks: sda, sdb & sdc. The OS is mounted on the sda, so we’ll leave that alone and make a raid 1 with sdb and sdc.

```
yum install mdadm
```
Assuming that the disks sdb & sdc are unmounted & unused, we can create a raid with the two of them by telling mdadm to create the partition:
```
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb /dev/sdc
```

If you have more disks, adjust the raid devices parameter and more devices listed at the end. Other levels of RAID can be setup as well like 0, 10, etc just by changing the –level parameter. So, if you were making a raid0 of 3 disks, you could call:
```
mdadm --create /dev/md0 --level=0 --raid-devices=3 /dev/sdb /dev/sdc /dev/sdd
```
You can confirm the setup by looking at mdstat:
```
cat /proc/mdstat
```
Now that we have a RAID array, they need to be monitored. This can be done through the mdmonitor service:
```
systemctl start mdmonitor
systemctl enable mdmonitor
```

####Making the file system & mounting it:
Next, we’ll create the file system that uses this raid array and call it /dev/md0 with the file system ext4
```
mkfs -t ext4 /dev/md0
```
You can now mount md0 to any folder you want. Like:
```
mount /dev/md0 /home/grumpyland
```
The new mount point should now appear when you call df:

We want the mount to be always there when we start up the server, so, we need to add it to fstab too using your favorite editor. I always found nano to be the most newbie friendly with the instruction on the bottom:
```
vi /etc/fstab
```
here, you’ll want to add a line about the md0 we just made. Note that the directory to be mounted must already exist:

```
/dev/md0                /home/grumpyland           ext4    defaults        0 0
```

#### Difference between Softwarde and hardware RAID:

The difference between software and hardware RAID is the location from where processing is performed. For software RAID processing is performed by the host server’s CPU (Central processing unit), whereas for hardware RAID processing is performed by an external CPU located on a dedicated RAID card.

Software RAID is configured and managed by the operating system of the host computer. All processing via software RAID is handled by the host computer’s CPU. Traditionally hardware RAID offered better I/O performance but with technological advancements offered on CPU and operating system technologies, there is very little difference in performance between hardware and software RAID.
